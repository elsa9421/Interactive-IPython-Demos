{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_ Application_Text_Generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN+wU50vjUf1eCXB9BAvjFS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elsa9421/Interactive-IPython-Demos/blob/main/RNN__Application_Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYBIlfWIQ4S4"
      },
      "source": [
        "## Application of RNN : to Generate text using character-based RNN\n",
        "\n",
        "Credit : Code similar to https://www.tensorflow.org/tutorials/text/text_generation \n",
        "\n",
        "\n",
        "This Notebook demostrates the use of different Recurrent Neural Networks (RNN) with Keras for Text Generation.\n",
        "\n",
        "There are three built-in RNN layers in Keras:\n",
        "\n",
        "* `keras.layers.SimpleRNN` :  a fully-connected RNN where the output from previous timestep is to be fed to next timestep.\n",
        "\n",
        "* `keras.layers.GRU`: first proposed in [Cho et al., 2014](https://arxiv.org/abs/1406.1078)\n",
        "\n",
        "* `keras.layers.LSTM`: first proposed in [Hochreiter & Schmidhuber, 1997](https://www.bioinf.jku.at/publications/older/2604.pdf)\n",
        "\n",
        "<br> The built-in `keras.layers.RNN`, `keras.layers.LSTM`, `keras.layers.GRU` layers enable you to quickly build recurrent models without having to make difficult configuration choices.\n",
        "<br> More about RNN with Keras [here](https://www.tensorflow.org/guide/keras/rnn)\n",
        "\n",
        "\n",
        "<br>  The model is trained on small batches of text (100 characters each), and is still able to generate a longer sequence of text with coherent structure.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcmGfnykWkpP"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7nWxEx_QxXs"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSYfxBAzcz1z"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8HAVKMRWqy1"
      },
      "source": [
        "\n",
        "## 1. Download Dataset\n",
        "\n",
        "Project Gutenberg is a library of over 60,000 free eBooks.You will find some of the world's greatest literature here, with focus on older works for which U.S. copyright has expired. Thousands of volunteers digitized and diligently proofread the eBooks, for enjoyment and education.\n",
        "These texts can be used to create generative models. [Project Gutenberg](https://www.gutenberg.org/)\n",
        "\n",
        "In this demo we use [Alice’s Adventures in Wonderland by Lewis Carroll](http://www.gutenberg.org/cache/epub/28885/pg28885.txt)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJOJCMKOYEa-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "e71e7ce9-3fa6-419b-cfda-97e1d25509ab"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('Alice_In_Wonderland.txt','http://www.gutenberg.org/cache/epub/28885/pg28885.txt')\n",
        "\n",
        "# Read, then decode for py3 compat. \n",
        "# Note that the text on the website  is UTF-8 encoded and \n",
        "# therefore we use the `decode` method to convert bytes to unicode code \n",
        "# points as we are reading data from a file into strings\n",
        "#Since Python 3.0, all strings are stored as Unicode in an instance of the str type Encoded strings on the other hand are represented as binary data in the form of instances of the bytes type.\n",
        "#Conceptually, str refers to text, whereas bytes refers to data. Use str.encode() to go from str to bytes, and bytes.decode() to go from bytes to str\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# the first 250 characters in text\n",
        "print(\" Printing the first 250 characters:\\n\",text[:250])\n",
        "print('Length of text: {} characters'.format(len(text)))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://www.gutenberg.org/cache/epub/28885/pg28885.txt\n",
            "180224/177428 [==============================] - 0s 1us/step\n",
            " Printing the first 250 characters:\n",
            " ﻿Project Gutenberg's Alice's Adventures in Wonderland, by Lewis Carroll\n",
            "\n",
            "This eBook is for the use of anyone anywhere at no cost and with\n",
            "almost no restrictions whatsoever.  You may copy it, give it away or\n",
            "re-use it under the terms of the Projec\n",
            "Length of text: 177412 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2gIODRkcfrV"
      },
      "source": [
        "##2. Process the data \n",
        "\n",
        "In order to prepare the data for modeling by the neural network. We cannot model the characters directly, instead we must convert the characters to integers. This can be done by :\n",
        "1. Create a set of all of the distinct characters in the book\n",
        "2. **Vectorize the Text** :Create two lookup tables: one mapping characters to numbers, and another for numbers to characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa3cIJ-1YWfr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "22988478-344c-4f14-c3f5-2ea3f8d86247"
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print ('\\nNo of unique characters={}'.format(len(vocab)))\n",
        "\n",
        "# Creating a dictionary for mapping from unique characters to indices\n",
        "char2idx = {ch:idx for idx, ch in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "# Represent each character in the text with the appropriate numeric code \n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "print(\"Total number of characters in text=\",len(text_as_int))\n",
        "\n",
        "# Show how the first 13 characters from the text are mapped to integers\n",
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "No of unique characters=89\n",
            "Total number of characters in text= 177412\n",
            "'\\ufeffProject Gute' ---- characters mapped to int ---- > [88 46 77 74 69 64 62 79  2 37 80 79 64]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUnxqzP1gWEo"
      },
      "source": [
        "## Create training examples and targets\n",
        "\n",
        "Next divide the text into example sequences. Each input sequence will contain `seq_length` characters from the text. This would correspond to the number of timesteps used for training per batch\n",
        "\n",
        "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
        "\n",
        "So break the text into chunks of `seq_length+1`. For example, say `seq_length` is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
        "\n",
        "To do this first use the `tf.data.Dataset.from_tensor_slices` function to convert the text vector into a stream of character indices.\n",
        "i.e creates a dataset with a separate element for each row of the input tensor:\n",
        "eg:-\n",
        "<br> `t = tf.constant([[1, 2], [3, 4]])`\n",
        "<br> `ds = tf.data.Dataset.from_tensor_slices(t)   # [1, 2], [3, 4]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48ceyy4Pe0W3"
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)  # Where each char is a separate tensor\n",
        "\n",
        "# for i in char_dataset.take(5):\n",
        "#   print(idx2char[i.numpy()])\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULSJBjWJi20X"
      },
      "source": [
        "The `batch` method lets us easily convert these individual characters to sequences of the desired size.\n",
        "\n",
        "And For each sequence, duplicate and shift it to form the input and target text by using the map method to apply a simple function to each batch\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLNCft2whHaW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "03395149-f4b1-4316-9d0f-9519be1e83de"
      },
      "source": [
        "\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)   #BatchDataset shapes: (101,)--> groups the tensor slices so that each batch has seq_length+1 tensors\n",
        "#print(len(list(sequences)))  # There are 1756 such batches\n",
        "# for i in sequences.take(1):\n",
        "#   print(repr(''.join(idx2char[i])))\n",
        "\n",
        "def split_input_target(chunk):\n",
        "  '''\n",
        "  Returns input_text, target_text for given chunk (sequence)\n",
        "\n",
        "  '''\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "\n",
        "dataset = sequences.map(split_input_target)  #((100,), (100,))  -->177412/101\n",
        "\n",
        "# Printing the examples and corresponding target data\n",
        "print(\"\\n\\n\")\n",
        "for input,target in dataset.take(1):\n",
        "   print(\"The Input sequence=\",repr(''.join(idx2char[input])))\n",
        "   print(\"The Target sequence=\",repr(''.join(idx2char[target])))\n",
        "   print(\"\\n\")\n",
        "\n",
        "\n",
        "# TO show input and  expected prediction at each time step\n",
        "\n",
        "for i, (input_idx, target_idx) in enumerate(zip(input[:5], target[:5])):\n",
        "    print(\"Time Step {:4d}\".format(i))\n",
        "    print(\"  Input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  Expected Output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "The Input sequence= \"\\ufeffProject Gutenberg's Alice's Adventures in Wonderland, by Lewis Carroll\\r\\n\\r\\nThis eBook is for the use\"\n",
            "The Target sequence= \"Project Gutenberg's Alice's Adventures in Wonderland, by Lewis Carroll\\r\\n\\r\\nThis eBook is for the use \"\n",
            "\n",
            "\n",
            "Time Step    0\n",
            "  Input: 88 ('\\ufeff')\n",
            "  Expected Output: 46 ('P')\n",
            "Time Step    1\n",
            "  Input: 46 ('P')\n",
            "  Expected Output: 77 ('r')\n",
            "Time Step    2\n",
            "  Input: 77 ('r')\n",
            "  Expected Output: 74 ('o')\n",
            "Time Step    3\n",
            "  Input: 74 ('o')\n",
            "  Expected Output: 69 ('j')\n",
            "Time Step    4\n",
            "  Input: 69 ('j')\n",
            "  Expected Output: 64 ('e')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSZwclltlddw"
      },
      "source": [
        "## Create training batches\n",
        "\n",
        "We used `tf.data` to split the text into manageable sequences i.e batches. But before feeding this data into the model, we need to shuffle the data and pack it into batches.\n",
        "\n",
        "`BATCH_SIZE` : Computations are normaly made in batches. The batch size is the number of training samples in one forward/backward pass. The higher the batch size, the more memory space you will need.\n",
        "<br/>Eg:\n",
        "<br/>  If there are `1000` training samples. And batch size is `100`, the algorithm takes the first `100 samples (0 to 100)` from the training dataset and train the network, then the next `100 samples (101 to 200)` and train the network again until the end.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nx0kBPF3kzDC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "de531b09-1ad0-4ac2-e2eb-5579dff0f227"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "# BUFFER_SIZE = 10000\n",
        "#dataset2 = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset2 = dataset.batch(BATCH_SIZE, drop_remainder=True)   # dataset2 ((64, 100), (64, 100)) --> 27 such sequences (1756/64)\n",
        "\n",
        "print(len( list(dataset2)))\n",
        "print(dataset2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27\n",
            "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXQeZ3fU-64t"
      },
      "source": [
        "## The model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vMaHH2zNNeD"
      },
      "source": [
        "### Define the different models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePt-MCF0_ZYD"
      },
      "source": [
        "Use `tf.keras.Sequential` to define the model. For this simple example three layers are used to define our model:\n",
        "\n",
        "* `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map the numbers of each character to a vector with `embedding_dim` dimensions;  Read more about Embedding layer [here](https://keras.io/api/layers/core_layers/embedding/#embedding)\n",
        "  <br> - input_dim: This is the size of the vocabulary in the text data. \n",
        "  <br> - output_dim: It defines the size of the output vectors from this layer  for each word. \n",
        "  <br> - input_length: This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000\n",
        "* `tf.keras.layers.GRU`: A type of RNN with size `units=rnn_units` (You can also use a LSTM layer here.) first proposed in [Cho et al., 2014](https://arxiv.org/abs/1406.1078)\n",
        "   <br> OR\n",
        "* `tf.keras.layers.SimpleRNN`  :   a fully-connected RNN where the output from previous timestep is to be fed to next timestep.\n",
        "   <br> OR\n",
        "* `tf.keras.layers.LSTM` :first proposed in [Hochreiter & Schmidhuber, 1997](https://www.bioinf.jku.at/publications/older/2604.pdf)\n",
        "\n",
        "* `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs.\n",
        "\n",
        "\n",
        "<br> For each character the model looks up the embedding, runs the GRU / LSTM /RNN one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqR7jbyVms-O"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n",
        "\n",
        "def build_model_GRU(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  '''\n",
        "  1. tf.keras.layers.Embedding :\n",
        "  Input shape:=(batch_size,None)\n",
        "  Output Shape:=(batch_size,None,vocab_size)\n",
        "\n",
        "  2. tf.keras.layers.GRU :\n",
        "  Input shape:= (batch_size,None,vocab_size)\n",
        "  Output shape=(batch_size,None,rnn_units)\n",
        "\n",
        "  3. tf.keras.layers.Dense :\n",
        "  Input shape=(batch_size,None,rnn_units)\n",
        "  Output shape=(batch_size, None, vocab_size) \n",
        "\n",
        "  '''\n",
        "  # Keras sequential model is used here since all the layers in the model only have single input and produce single output.\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),  #embedding_dim*vocab_size=trainable parameters\n",
        "    tf.keras.layers.GRU(rnn_units,                                    # No of feed forward networks g = 3 (GRU has 3 FFNNs)\n",
        "                        return_sequences=True,                        # Number of trainable parameters= # g × [h(h+i) + h] => 3x[1024(1024+256)+1024]\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)                                #number of parameters in Dense is rnn_units*vocab_size+vocab_size\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "def build_model_SimpleRNN(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),   #embedding_dim*vocab_size=trainable parameters\n",
        "    tf.keras.layers.SimpleRNN(rnn_units,return_sequences=True,        #number of parameters in simple RNN layer is rnn_units * (rnn_units + embedding_dim + 1)\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)                                 #number of parameters in Dense is rnn_units*vocab_size+vocab_size\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "def build_model_LSTM(vocab_size, embedding_dim, rnn_units, batch_size):    #[for LSTM number of feed forward networks =4 i.e g=4]\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),      # No. of trainable parameters= sembedding_dim*vocab_size\n",
        "    tf.keras.layers.LSTM(rnn_units,                                       # g × [h(h+i) + h] => 4x[1024(1024+256)+1024]\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)                                     #number of parameters in Dense is rnn_units*vocab_size+vocab_size\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "\n",
        "model_GRU = build_model_GRU(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)\n",
        "\n",
        "model_SimpleRNN = build_model_SimpleRNN(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)\n",
        "\n",
        "model_LSTM = build_model_LSTM(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPUuj6AwCIcW"
      },
      "source": [
        "# model_GRU.summary()\n",
        "\n",
        "# model_SimpleRNN.summary()\n",
        "# model_LSTM.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5svX6YskDDI8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "83874820-7898-4eee-d2fa-bb8f04324eae"
      },
      "source": [
        "# Try the model\n",
        "\n",
        "for input_example_batch, target_example_batch in dataset2.take(1):\n",
        "  #print(input_example_batch.shape)  #(64,100)\n",
        "  example_batch_predictions = model_LSTM(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "  example_batch_predictions = model_SimpleRNN(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "  example_batch_predictions = model_GRU(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "#This gives us, at each timestep, a prediction of the next character index:\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)   # picks one from most likely sample from every row i.e (100,1) based on logits\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()   #(100,)\n",
        "\n",
        "\n",
        "#Decode these to see the text predicted by this untrained model:\n",
        "\n",
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 89) # (batch_size, sequence_length, vocab_size)\n",
            "(64, 100, 89) # (batch_size, sequence_length, vocab_size)\n",
            "(64, 100, 89) # (batch_size, sequence_length, vocab_size)\n",
            "Input: \n",
            " \"\\ufeffProject Gutenberg's Alice's Adventures in Wonderland, by Lewis Carroll\\r\\n\\r\\nThis eBook is for the use\"\n",
            "\n",
            "Next Char Predictions: \n",
            " 'fVh\\nuO;$36mz4]JT7]8@EI:MpO*T[5·P&Ov_,vRQRo0Y30Al_9I9Up-Te$:H1m\\ufeff10P\\ufeffitcMA!%Y7BQ.QqQj#g]nIw#[iD?]MwvDF'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca9YHY2sNDOa"
      },
      "source": [
        "### Optimizer and Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TGkXeFfKm0n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "30b7b42c-af2a-4612-9733-9ca6f4042197"
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "  #return tf.keras.losses.categorical_crossentropy(labels, logits, from_logits=False)\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n",
        "\n",
        "model_SimpleRNN.compile(optimizer='adam', loss=loss)\n",
        "model_GRU.compile(optimizer='adam', loss=loss)\n",
        "model_LSTM.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 89)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.4923635\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_Bk1WoNNU_t"
      },
      "source": [
        "### Create checkpoint to save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVBr6_XFNR-l"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir_GRU = './training_checkpoints_GRU'\n",
        "checkpoint_dir_RNN='./training_checkpoints_RNN'\n",
        "checkpoint_dir_LSTM='./training_checkpoints_LSTM'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix_GRU = os.path.join(checkpoint_dir_GRU, \"ckpt_{epoch}\")\n",
        "checkpoint_prefix_RNN = os.path.join('./training_checkpoints_RNN', \"ckpt_{epoch}\")\n",
        "checkpoint_prefix_LSTM = os.path.join('./training_checkpoints_LSTM', \"ckpt_{epoch}\")\n",
        "\n",
        "\n",
        "## change filepath to select particular model\n",
        "checkpoint_callback_RNN=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix_RNN,\n",
        "    save_weights_only=True)\n",
        "\n",
        "## change filepath to select particular model\n",
        "checkpoint_callback_GRU=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix_GRU,\n",
        "    save_weights_only=True)\n",
        "\n",
        "checkpoint_callback_LSTM=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix_LSTM,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QEFWHjvNchM"
      },
      "source": [
        "## Training\n",
        "### Train the model \n",
        "\n",
        "To keep training time reasonable, use 10 epochs to train the model. In Colab, set the runtime to GPU for faster training.\n",
        "\n",
        "UNCOMMENT THE FOLLOWING LINES TO TRAIN. Please note, you can also load saved checkpoints from GitHub, see next code block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QN0gDGguNi8r"
      },
      "source": [
        "# EPOCHS=30\n",
        "# %time history_RNN = model_SimpleRNN.fit(dataset2, epochs=EPOCHS, callbacks=[checkpoint_callback_RNN])\n",
        "# %time history_GRU = model_GRU.fit(dataset2, epochs=EPOCHS, callbacks=[checkpoint_callback_GRU])\n",
        "# %time history_LSTM= model_LSTM.fit(dataset2, epochs=EPOCHS, callbacks=[checkpoint_callback_LSTM])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQAydgb8wlDl"
      },
      "source": [
        " COMMENT THE FOLLOWING IF YOU HAVE TRAINED THE MODEL\n",
        "\n",
        " The checkpoints for the 30th epoch have been save in GitHub, loading the checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFDGF_2owJul",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        },
        "outputId": "12caf353-e9ce-4b87-acfd-ec6e98e33d73"
      },
      "source": [
        " # UNCOMMENT THE FOLLOWING LINES TO INSTALL git lfs so that large files can be downloaded from GitHub!!!!\n",
        " # This is used as Checkpoints are > 25MB\n",
        "###########\n",
        "\n",
        "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
        "!sudo apt-get install git-lfs\n",
        "!git lfs install\n",
        "\n",
        "##########\n",
        "!git  clone https://github.com/EECS545-FA2020/RNN_Checkpoints.git Checkpoint\n",
        "\n",
        "# # load the network weights\n",
        "checkpoint_dir_RNN = \"/content/Checkpoint/SimpleRNN/\"\n",
        "\n",
        "checkpoint_dir_GRU = \"/content/Checkpoint/GRU/\"\n",
        "checkpoint_dir_LSTM = \"/content/Checkpoint/LSTM/\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Detected operating system as Ubuntu/bionic.\n",
            "Checking for curl...\n",
            "Detected curl...\n",
            "Checking for gpg...\n",
            "Detected gpg...\n",
            "Running apt-get update... done.\n",
            "Installing apt-transport-https... done.\n",
            "Installing /etc/apt/sources.list.d/github_git-lfs.list...done.\n",
            "Importing packagecloud gpg key... done.\n",
            "Running apt-get update... done.\n",
            "\n",
            "The repository is setup! You can now install packages.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  git-lfs\n",
            "0 upgraded, 1 newly installed, 0 to remove and 66 not upgraded.\n",
            "Need to get 6,877 kB of archives.\n",
            "After this operation, 16.4 MB of additional disk space will be used.\n",
            "Get:1 https://packagecloud.io/github/git-lfs/ubuntu bionic/main amd64 git-lfs amd64 2.11.0 [6,877 kB]\n",
            "Fetched 6,877 kB in 1s (8,497 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package git-lfs.\n",
            "(Reading database ... 144469 files and directories currently installed.)\n",
            "Preparing to unpack .../git-lfs_2.11.0_amd64.deb ...\n",
            "Unpacking git-lfs (2.11.0) ...\n",
            "Setting up git-lfs (2.11.0) ...\n",
            "Git LFS initialized.\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Git LFS initialized.\n",
            "Cloning into 'Checkpoint'...\n",
            "remote: Enumerating objects: 25, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 25 (delta 4), reused 17 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (25/25), done.\n",
            "Filtering content: 100% (3/3), 123.26 MiB | 77.43 MiB/s, done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksWQqtYSNzyl"
      },
      "source": [
        "### Restore model from checkpoint\n",
        "\n",
        "To keep this prediction step simple, use a batch size of 1.\n",
        "\n",
        "Because of the way the RNN state is passed from timestep to timestep, the model only accepts a fixed batch size once built.\n",
        "\n",
        "To run the model with a different `batch_size`, we need to rebuild the model and restore the weights from the checkpoint.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVrYhHk8NxCT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 974
        },
        "outputId": "e9627719-23f1-4c1a-c878-2469c49e139d"
      },
      "source": [
        "model_SimpleRNN = build_model_SimpleRNN(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model_SimpleRNN.load_weights(tf.train.latest_checkpoint(checkpoint_dir_RNN))\n",
        "\n",
        "model_SimpleRNN.build(tf.TensorShape([1, None]))\n",
        "\n",
        "\n",
        "model_GRU = build_model_GRU(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model_GRU.load_weights(tf.train.latest_checkpoint(checkpoint_dir_GRU))\n",
        "\n",
        "model_GRU.build(tf.TensorShape([1, None]))\n",
        "\n",
        "\n",
        "\n",
        "model_LSTM = build_model_LSTM(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model_LSTM.load_weights(tf.train.latest_checkpoint(checkpoint_dir_LSTM))\n",
        "\n",
        "model_LSTM.build(tf.TensorShape([1, None]))\n",
        "\n",
        "print(\"Model_SimpleRNN Summary\\n\")\n",
        "model_SimpleRNN.summary()\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "print(\"Model_GRU Summary\\n\")\n",
        "model_GRU.summary()\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "print(\"Model_LSTM Summary\\n\")\n",
        "model_LSTM.summary()\n",
        "print(\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model_SimpleRNN Summary\n",
            "\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (1, None, 256)            22784     \n",
            "_________________________________________________________________\n",
            "simple_rnn_1 (SimpleRNN)     (1, None, 1024)           1311744   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (1, None, 89)             91225     \n",
            "=================================================================\n",
            "Total params: 1,425,753\n",
            "Trainable params: 1,425,753\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "\n",
            "\n",
            "Model_GRU Summary\n",
            "\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (1, None, 256)            22784     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (1, None, 89)             91225     \n",
            "=================================================================\n",
            "Total params: 4,052,313\n",
            "Trainable params: 4,052,313\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "\n",
            "\n",
            "Model_LSTM Summary\n",
            "\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (1, None, 256)            22784     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (1, None, 1024)           5246976   \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (1, None, 89)             91225     \n",
            "=================================================================\n",
            "Total params: 5,360,985\n",
            "Trainable params: 5,360,985\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOgK9oY6OzLd"
      },
      "source": [
        "## Generate Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ44WWqCOiCT"
      },
      "source": [
        "def generate_text(model, start_string,num_generate):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  #num_generate = 100\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]   #[len(start_string)]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)         #(1,len(start_string))\n",
        "  #print(input_eval.shape)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  if model=='SimpleRNN':\n",
        "    model=model_SimpleRNN\n",
        "    print(\"\\n For Simple RNN:\\n\\n\")\n",
        "  elif model=='GRU':\n",
        "    model=model_GRU\n",
        "    print(\"\\n For GRU:\\n\\n\")\n",
        "  elif model=='LSTM':\n",
        "    model=model_LSTM\n",
        "    print(\"\\n For LSTM:\\n\\n\")\n",
        "  model.reset_states()\n",
        "  print(start_string,end=\"\")\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "      print(idx2char[predicted_id],end=\"\")\n",
        "      \n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxRLtVjCOiwQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739
        },
        "outputId": "bd74226f-d3ba-488e-d4cf-4a444300c0a4"
      },
      "source": [
        "\n",
        "types_m=['SimpleRNN','GRU','LSTM']\n",
        "for m in types_m:\n",
        "  print(\"\\n\",\"_\"*50)\n",
        "  generate_text(model=m, start_string=\"Open the window\",num_generate=200)\n",
        "  print(\"\\n\",\"_\"*50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " __________________________________________________\n",
            "\n",
            " For Simple RNN:\n",
            "\n",
            "\n",
            "Open the window, she such hir. IY, addean, soister, so hard every Every,  if you,\" If I het kild!\"\n",
            "\n",
            "The F ceragis.\n",
            "\n",
            "[Illuste;\"-the King.\n",
            "\n",
            "Grese, herpent-exs\n",
            "bleash, so shilk Bill himply\n",
            "kreapure, so it a ver\n",
            " __________________________________________________\n",
            "\n",
            " __________________________________________________\n",
            "\n",
            " For GRU:\n",
            "\n",
            "\n",
            "Open the window,\"\n",
            "\n",
            "\"Hander dimesty furrieds, \"you indeed\n",
            "to for, then!\"\n",
            "\n",
            "\"Stupid eyes very decause he\n",
            "surpered tone, \"she's must be reach nobsiggland by the court,_ as the Gryphon reperied im distribution\n",
            "rom\n",
            " __________________________________________________\n",
            "\n",
            " __________________________________________________\n",
            "\n",
            " For LSTM:\n",
            "\n",
            "\n",
            "Open the window, Hatter brimbed and-bringied in\n",
            "helf unto a pack in or,\" he began Fromouting fir the rest ard such\n",
            "it almstards,) the said just not that her varined tire\n",
            "she spake at up liffering violently\n",
            "prope\n",
            " __________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}