{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elsa9421/Interactive-IPython-Demos/blob/main/Reinforcement_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0agNC1XIvsH"
      },
      "source": [
        "This notebook demonstrates Reinforcement Learning specifically, by solving of a Markov Decision Process(MDP) using the Bellman Equation and Dynamic Programming.\n",
        "\n",
        "<br>The Bellman Equation is the basic block of Reinforcement Learning. We can solve the Bellman equation using a special technique called dynamic programming.\n",
        "<br>There are two powerful algorithms under Dynamic Programming\n",
        "- Value Iteration\n",
        "- Policy Iteration\n",
        "\n",
        "<br>This notebook demonstrates solving of the Taxi Game and Frozen Lake problem using \"Value Iteration\"\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3i-y37KKKfF0"
      },
      "source": [
        "## References/ Links\n",
        "\n",
        "- [Learn by Example Reinforcement Learning Taxi Game](https://www.kaggle.com/charel/learn-by-example-reinforcement-learning-with-gym#The-Taxi-game)\n",
        "- [Introduction to Reinforcement Learning and OpenAI Gym](https://www.oreilly.com/radar/introduction-to-reinforcement-learning-and-openai-gym/)\n",
        "- [Bellman Equation and Dynamic Programming](https://medium.com/analytics-vidhya/bellman-equation-and-dynamic-programming-773ce67fc6a7)\n",
        "- [Solving FrozenLake](https://medium.com/analytics-vidhya/solving-the-frozenlake-environment-from-openai-gym-using-value-iteration-5a078dffe438)\n",
        "-[FrozenLake8x8-v0](https://gym.openai.com/envs/FrozenLake8x8-v0/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjA8tMfhJ8nq"
      },
      "source": [
        "## Basics \n",
        "\n",
        "### Gym\n",
        "Gym is released by Open AI in 2016 [Read more](http://gym.openai.com/docs/). It is a toolkit for developing and comparing reinforcement learning algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtKFSpl_IppU"
      },
      "source": [
        "## Import\n",
        "\n",
        "import gym # openAi gym\n",
        "from gym import envs\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ3xsZisjddE"
      },
      "source": [
        "### Example 1: Taxi Game\n",
        "\n",
        "<br> Problem Description :\n",
        "\n",
        "\n",
        "1. **Rules**: \n",
        "* There are four designated locations in the grid world indicated by R(ed) , B(lue),  G(reen) ,Y(ellow)\n",
        "* When the episode starts, the taxi starts off at a random square and the passenger is at a random location.\n",
        "* The taxi drives to the passenger's location, picks up the passenger, drives to the passenger's destination (another one of the four specified locations), and then drops off the passenger. \n",
        "* Once the passenger is dropped off, the episode ends. \n",
        "* The taxi cannot pass through a wall.\n",
        "\n",
        "2. **Actions** `a`: \n",
        "There are 6 discrete deterministic actions:\n",
        ">>- 0: move south\n",
        "  - 1: move north\n",
        "  - 2: move east \n",
        "  - 3: move west \n",
        "  - 4: pickup passenger\n",
        "  - 5: dropoff passenger\n",
        "3. **Rewards** : \n",
        ">>- There is a reward of -1 for each action \n",
        "  - An additional reward of +20 for delievering the passenger. \n",
        "  - There is a reward of -10 for executing actions \"pickup\" and \"dropoff\" illegally.\n",
        "\n",
        "4. **Illustration description/Rendering**:\n",
        ">>- blue: passenger\n",
        "  - magenta: destination\n",
        "  - yellow: empty taxi\n",
        "  - green: full taxi\n",
        "  - other letters: locations\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEjYn1Kelyh-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "d6791583-b86a-4e02-9212-b97b41bd1c1e"
      },
      "source": [
        "env = gym.make('Taxi-v3')\n",
        "env.reset()\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bF5QII2mt7a"
      },
      "source": [
        "## Iteracting with the Gym Environment\n",
        "\n",
        "\n",
        "<br> At each timestep, the agent chooses an action, and the environment returns an observation and a reward.\n",
        "\n",
        "<br> `observation, reward, done, info = env.step(action)`\n",
        "where, \n",
        "<br>`observation` (object): an environment-specific object representing your observation of the environment. \n",
        "<br>`reward` (float): amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward.\n",
        "<br>`done` (boolean): whether it’s time to reset the environment again. Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. \n",
        "<br>`info` (dict): ignore, diagnostic information useful for debugging. Official evaluations of your agent are not allowed to use this for learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqZMNKecnXPs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "outputId": "2d204f5b-97d6-4bfa-bc1f-d79ef4f41cdb"
      },
      "source": [
        "# Let's first do some random steps in the game so you see how the game looks like\n",
        "\n",
        "reward_tot=0\n",
        "obs= env.reset()\n",
        "env.render()\n",
        "for _ in range(3):\n",
        "    action = env.action_space.sample() #take step using random action from possible actions (action_space)\n",
        "    obs, rew, done, info = env.step(action) \n",
        "    reward_tot = reward_tot + rew\n",
        "    env.render()\n",
        "#Print the reward of these random action\n",
        "print(\"Reward: %r\" % reward_tot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
            "|\u001b[43m \u001b[0m: | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "|\u001b[43m \u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[43m \u001b[0m| : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[43m \u001b[0m| : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "Reward: -12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl5kVXd5oZwr"
      },
      "source": [
        "## Action Space for the Taxi Game :\n",
        "Action space has 6 possible actions, the meaning of the actions is nice to know for us humans but the neural network will figure it out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pjy9TsT0oZE8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "4b98ec04-ed69-4dc5-e829-e8fdb2957043"
      },
      "source": [
        "print(env.action_space)\n",
        "NUM_ACTIONS = env.action_space.n\n",
        "print(\"Possible actions: [0..%a]\" % (NUM_ACTIONS-1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discrete(6)\n",
            "Possible actions: [0..5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TmJpy60ovOH"
      },
      "source": [
        "## State :\n",
        "\n",
        "This represents the board state of the game and in gym returned it is returned as `observation`. It is a numeric representation of what the agent is observing at a particular moment of time in the environment.\n",
        "<br> In case of Taxi the observation is an integer, 500 different states are possible that translate to a nice graphic visual format with the `render` function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOsSiLeTpAc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "4767cedd-bc33-4893-9604-0923b486374e"
      },
      "source": [
        "print(env.observation_space)\n",
        "print()\n",
        "env.env.s=42 # some random number, you might recognize it\n",
        "env.render()\n",
        "env.env.s = 222 # and some other\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discrete(500)\n",
            "\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: |\u001b[43m \u001b[0m: :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[43m \u001b[0m: : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Pickup)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2qm_1k3pNRd"
      },
      "source": [
        "## Policy(π): \n",
        "\n",
        "It is the probability distribution over actions. i.e $\\pi(a/s)$ - action picked by the agent given a state.\n",
        "\n",
        "In this case, it is a deterministic policy\n",
        "$a=\\pi(s)$\n",
        "\n",
        "The strategy that the agent employs to determine next action `a` in state `s`. Note that it does not state if it is a good or bad policy, it is a policy. The policy is normally noted with the greek letter π. Optimal policy (π*), policy which maximizes the expected reward.\n",
        "\n",
        "<br> We use the Bellman Equation to find the optimal policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XAgGx1ZMO71"
      },
      "source": [
        "## Bellman Equation \n",
        "\n",
        "<br> Bellman equation is the basic block of solving reinforcement learning and is omnipresent in RL. It helps us to solve MDP. To solve means finding the optimal policy and value functions.\n",
        "\n",
        "> A) For Deterministic Environment :\n",
        "  <br> $V^*(s)= \\underset{a}{\\operatorname{max}} \\{\\space R(s,a) + \\gamma\\space \\space V^*(s')\\space \\} $\n",
        "\n",
        "\n",
        "> B) For Stochastic Environment :\n",
        "   $V^*(s)= \\underset{a}{\\operatorname{max}} \\underset{s'}{\\operatorname{\\Sigma}}\\space  P(s'|s,a) \\{\\space R(s,a,s') + \\gamma\\space V^*(s')\\space \\} $\n",
        "\n",
        "Note :\n",
        "Optimal policy\n",
        "   $\\pi^*(s)= \\underset{a}{\\operatorname{argmax}} \\underset{s'}{\\operatorname{\\Sigma}} P(s'|s,a) \\{\\space R(s,a,s') + \\gamma\\space \\space  V^*(s')\\space \\} $\n",
        "\n",
        "\n",
        "where,\n",
        "<br> $V(s)$ :  is the value for being in a certain state `s`;\n",
        "$V^*(s)$ is the optimal value function the one that yields maximum value\n",
        "<br> $V*(s')$ :  is the optimal value for being in the next state `s'`after taking action `a` ;\n",
        "<br> $R(s,a)$ : Reward obtained on taking  action `a` in state `s` and reachinf state s'\n",
        "<br> $P(s'|s,a)$ :  Probability of going to state `s'` given action `a` is performed in state `s`.\n",
        "<br>he Taxi game actions are deterministic (no such a thing as if I want to go north there is an 80% chance to go north and 10% chance to go west and 10% chance to go east). so the probability that selected action will lead to expected state is 100%. So ignore it for this game, it is always 1.\n",
        "<br> $γ$ : Discount factor gamma; Between 0 and 1; Indicates importance to be given to future rewards.The higher gamma the higher the focus on long term rewards\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4visBRUIjYv1"
      },
      "source": [
        "\n",
        "### Value Iteration Algorithm \n",
        "\n",
        "1. Start with $V_0^*(s)$ =0 for all $s$\n",
        "2. While not converged :\n",
        "> For each state s, Given $V_i^*$, calculate $V_{i+1}^*$\n",
        "$V_{i+1}^*(s)= \\underset{a}{\\operatorname{max}} \\underset{s'}{\\operatorname{\\Sigma}}\\space  P(s'|s,a) \\{\\space R(s,a,s') + \\gamma\\space V_i^*(s')\\space \\} $\n",
        "<br> This is called value update or Bellman update/back-up\n",
        "3. After Value Iteration , we still need to extract the optimal policy to obtain optimal action taken in each state\n",
        "$\\pi^*(s)= \\underset{a}{\\operatorname{argmax}} \\underset{s'}{\\operatorname{\\Sigma}}\\space  P(s'|s,a)\\{\\space R(s,a,s') + \\gamma\\space V^*(s')\\space \\} $\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuancMOCrB8Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "87ca1c65-8ce2-4fcc-d1a5-752adf0ff1ad"
      },
      "source": [
        "# Value iteration algorithem\n",
        "NUM_ACTIONS = env.action_space.n\n",
        "NUM_STATES = env.observation_space.n\n",
        "V = np.zeros([NUM_STATES]) # The Value for each state\n",
        "Pi = np.zeros([NUM_STATES], dtype=int)  # Our policy with we keep updating to get the optimal policy\n",
        "gamma = 0.9 # discount factor\n",
        "significant_improvement = 0.01\n",
        "\n",
        "def best_action_value(s):\n",
        "    # finds the highest value action (max_a) in state s\n",
        "    best_a = None\n",
        "    best_value = float('-inf')\n",
        "\n",
        "    # loop through all possible actions to find the best current action\n",
        "    for a in range (0, NUM_ACTIONS):\n",
        "        env.env.s = s\n",
        "        s_new, rew, done, info = env.step(a) #take the action\n",
        "        v = rew + gamma * V[s_new]\n",
        "        if v > best_value:\n",
        "            best_value = v\n",
        "            best_a = a\n",
        "    return best_a\n",
        "\n",
        "iteration = 0\n",
        "while True:\n",
        "\n",
        "    biggest_change = 0\n",
        "    for s in range (0, NUM_STATES):\n",
        "        old_v = V[s]\n",
        "        action = best_action_value(s) #choosing an action with the highest future reward\n",
        "        env.env.s = s # goto the state\n",
        "        s_new, rew, done, info = env.step(action) #take the action\n",
        "        V[s] = rew + gamma * V[s_new] #Update Value for the state using Bellman equation\n",
        "        Pi[s] = action\n",
        "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
        "    iteration += 1\n",
        "    if biggest_change < significant_improvement:\n",
        "        print (iteration,' iterations done')\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "41  iterations done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rk2Av8G7yGos"
      },
      "source": [
        "## Solution to Taxi Game :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlG_DFnRrItH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f5f4a72d-1a90-4520-f27e-37cc497b0d84"
      },
      "source": [
        "\n",
        "rew_tot=0\n",
        "obs= env.reset()\n",
        "env.render()\n",
        "done=False\n",
        "while done != True: \n",
        "    action = Pi[obs]\n",
        "    obs, rew, done, info = env.step(action) #take step using selected action\n",
        "    rew_tot = rew_tot + rew\n",
        "    env.render()\n",
        "#Print the reward of these actions\n",
        "print(\"Reward: %r\" % rew_tot)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | :\u001b[43m \u001b[0m| : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : |\u001b[43m \u001b[0m: : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: |\u001b[43m \u001b[0m: :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | :\u001b[43m \u001b[0m:\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :\u001b[42mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : :\u001b[42m_\u001b[0m|\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : :\u001b[42m_\u001b[0m|\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[42m_\u001b[0m: |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[42m_\u001b[0m: : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[42m_\u001b[0m: : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[42m_\u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[42m_\u001b[0m| : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[42mY\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "Reward: 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2lRc1yh1Wev"
      },
      "source": [
        "## Example 2: FrozenLake4x4\n",
        "\n",
        "* There are 16 states in the game. \n",
        "* The agent starts from S (S for Start) and our goal is to get to G (Goal)\n",
        "* F means Frozen Surface. You can walk on them. \n",
        "* But H means Hole. If you fall in a H, and start from S again.\n",
        "* Since this is a “Frozen” Lake, if you go in a certain direction, there is only 0.333% chance that the agent will really go in that direction. The movement of the agent is uncertain and only partially depends on the chosen direction. \n",
        "\n",
        "Use Stochastic form of Bellman equation to solve this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeX5BBXV21bB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "7f6944ae-b64e-4cc2-f3c1-da5b6257b37b"
      },
      "source": [
        "env=gym.make('FrozenLake-v0')\n",
        "env.render()\n",
        "print(\"Number of Actions in Action Space A : \",env.action_space)\n",
        "print(\"Number of States in States Space S :\",env.observation_space)\n",
        "#OR\n",
        "print(env.nS)\n",
        "print(env.nA)\n",
        "\n",
        "len(env.P[0][1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Number of Actions in Action Space A :  Discrete(4)\n",
            "Number of States in States Space S : Discrete(16)\n",
            "16\n",
            "4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o9lauG26kFr"
      },
      "source": [
        "env.P ; \n",
        "`env.P[state]` :\n",
        "<br> eg. env.P[0] outputs a dictionary as shown in code below. \n",
        "<br>Here 0 in env.P[0] is the first state of the environment.<br> The keys of the dictionary 0,1,2,3 are the actions we can state from state 0. And further each action contains a list, where each element of the list is a tuple showing the `probability of transitioning into the state`, `next state`, `reward` and  if `done`=True `done`=False. (done=True if the next state is a Hole or the Goal)]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEx4mBbw6uen",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "23fbe24c-cdb8-4d6f-dd58-26eff2bb07c3"
      },
      "source": [
        "print(env.P[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)], 1: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 1, 0.0, False)], 2: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False)], 3: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False)]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3pG6ewr3194",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "54f06f41-03c9-4e7e-a4ac-7f9601a1b1e1"
      },
      "source": [
        "env = gym.make('FrozenLake-v0')\n",
        "env.render()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7y3Qg2OFzOc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0fc07bec-e611-4f6a-c054-296b9be73ce2"
      },
      "source": [
        "# Value iteration algorithem\n",
        "NUM_ACTIONS = env.nA\n",
        "NUM_STATES = env.nS\n",
        "V = np.zeros([NUM_STATES]) # The Value for each state\n",
        "Pi = np.zeros([NUM_STATES], dtype=int)  # Our policy with we keep updating to get the optimal policy\n",
        "gamma = 0.9 # discount factor\n",
        "significant_improvement = 0.01\n",
        "\n",
        "def best_action_value(s):\n",
        "    # finds the highest value action (max_a) in state s\n",
        "\n",
        "    stateValue = [0 for i in range(env.nS)]\n",
        "    action_values=[]\n",
        "\n",
        "    # loop through all possible actions to find the best current action\n",
        "    for a in range (0, NUM_ACTIONS):\n",
        "      state_value = 0\n",
        "      for i in range(len(env.P[s][a])):\n",
        "        #env.env.s = s  env.P[s][action]\n",
        "        prob,s_new, rew, done = env.P[s][a][i] \n",
        "        state_action_value=prob*( rew + gamma * V[s_new])\n",
        "        state_value += state_action_value\n",
        "      action_values.append(state_value)      #the value of each action\n",
        "      best_action = np.argmax(np.asarray(action_values))\n",
        "\n",
        "    return best_action\n",
        "\n",
        "iteration = 0\n",
        "while True:\n",
        "\n",
        "    biggest_change = 0\n",
        "    for s in range (0, NUM_STATES):\n",
        "      old_v = V[s]\n",
        "      action = best_action_value(s) #choosing an action with the highest future reward\n",
        "      action_value=0\n",
        "      for i in range(len(env.P[s][action])):\n",
        "        prob, s_new,rew, done, =  env.P[s][action][i]\n",
        "        action_value+= (prob * (rew + gamma * V[s_new])) #Update Value for the state using Bellman equation\n",
        "      V[s]=action_value\n",
        "      Pi[s] = action\n",
        "      biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
        "    iteration += 1\n",
        "    if biggest_change < significant_improvement:\n",
        "        print (iteration,' iterations done')\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10  iterations done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eINC7bKHyk4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "51a59f92-af87-401c-e5a6-8800cd994550"
      },
      "source": [
        "\n",
        "rew_tot=0\n",
        "obs= env.reset()\n",
        "env.render()\n",
        "done=False\n",
        "while done != True: \n",
        "    action = Pi[obs]\n",
        "    obs, rew, done, info = env.step(action) #take step using selected action\n",
        "    rew_tot = rew_tot + rew\n",
        "    env.render()\n",
        "#Print the reward of these actions\n",
        "print(\"Reward: %r\" % rew_tot)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Reward: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}